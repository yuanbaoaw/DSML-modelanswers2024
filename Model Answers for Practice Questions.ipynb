{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9c0b94b",
   "metadata": {},
   "source": [
    "## Practice Questions\n",
    "\n",
    "This notebook contains practise questions for the assess coursework on the 19th of March. I will upload some model answers in a later lecture. If you can answer all of these questions, then you shouldn't have too much trouble in the coursework.\n",
    "\n",
    "Please use the ‘litho_log’ data available in the data folder of this repository to complete these exercises.\n",
    "\n",
    "### Exercise 1 (Approx. 15 mins)\n",
    "\n",
    "You have been given some data that contains a large number of observations of downhole logs and the name of the lithologies associated with the log response.\n",
    " - 'DEPTH_WMSF': the depth of the measurement below seafloor \n",
    " - 'HCGR': Total gamma ray counts \n",
    " - 'HFK': Potassium counts \n",
    " - 'HTHO': Thorium counts \n",
    " - 'HURA': Uranium counts \n",
    " - 'IDPH': Deep Phasor Dual Induction–Spherically Focused Resistivity \n",
    " - 'IMPH': Medium Phasor Dual Induction–Spherically Focused Resistivity \n",
    " - 'SFLU': Shallow Phasor Dual Induction–Spherically Focused Resistivity \n",
    " - 'lithology': our target value, a string representing the name of the lithology\n",
    " \n",
    "Using a Markdown cell, describe the steps that you would take to clean this data and prepare it for machine learning analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f935aa68",
   "metadata": {},
   "source": [
    "### Write your answer here (in this Markdown cell)\n",
    "\n",
    "Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd1da7c",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "I would do the following:\n",
    "\n",
    " - Remove duplicate data\n",
    " - Split data into features and target variable\n",
    "     - Because our target variable uses strings, it should be encoded into numbers after splitting.\n",
    " - Create a train-test split\n",
    " - Inspect the data for unusual values\n",
    " - Drop/reassign unusual values depending on what they mean\n",
    " - Create a pipeline (or function) to train an Imputer and a Scaler to remove null values and to scale the data\n",
    " \n",
    "Bonus points if you indicate which steps can be combined into functions. Also note that the order in which you clean and prepare the data is important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376257ae",
   "metadata": {},
   "source": [
    "### Exercise 2 (25 minutes)\n",
    "\n",
    "Load the data set and drop any duplicates you find.\n",
    "\n",
    "Then answer the following questions:\n",
    "\n",
    " - What is the distribution of the lithologies in this dataset?\n",
    " - What is the average depth of the interbedded clay and mud?\n",
    " - Among the samples found at or below 400m (below seafloor), what are the characteristics of the samples with the five highest Uranium counts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e764111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"Data/litho_log_data.csv\")\n",
    "\n",
    "# Drop duplicates\n",
    "data.drop_duplicates(inplace = True)\n",
    "\n",
    "# Check there are no duplicates remaining\n",
    "print(data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46707d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Use value counts to see the distribution\n",
    "print(data['lithology'].value_counts())\n",
    "\n",
    "# Part 2: Subselect Interbedded clays and muds, then find the mean of the DEPTH column\n",
    "display(data[data['lithology'] == 'Interbedded clay and mud']['DEPTH_WMSF'].mean())\n",
    "\n",
    "# Part 3: Subselect samples below 400m, sort by descending HURA, return the top 5 values\n",
    "data[data['DEPTH_WMSF'] >= 400].sort_values(by = 'HURA', ascending = False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4483fc99",
   "metadata": {},
   "source": [
    "### Exercise 3.1 (10 minutes)\n",
    "\n",
    "Using the steps you outlined in Exercise 1, split this dataset into a training set and a testing set (with reasonable names). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f4b33b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEPTH_WMSF</th>\n",
       "      <th>HCGR</th>\n",
       "      <th>HFK</th>\n",
       "      <th>HTHO</th>\n",
       "      <th>HURA</th>\n",
       "      <th>IDPH</th>\n",
       "      <th>IMPH</th>\n",
       "      <th>SFLU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7632</th>\n",
       "      <td>58.8988</td>\n",
       "      <td>11.0347</td>\n",
       "      <td>0.3173</td>\n",
       "      <td>1.6807</td>\n",
       "      <td>0.5729</td>\n",
       "      <td>1950.0000</td>\n",
       "      <td>1765.0116</td>\n",
       "      <td>0.0915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11425</th>\n",
       "      <td>327.2764</td>\n",
       "      <td>50.1788</td>\n",
       "      <td>1.2883</td>\n",
       "      <td>8.5476</td>\n",
       "      <td>1.9532</td>\n",
       "      <td>1.2876</td>\n",
       "      <td>1.2937</td>\n",
       "      <td>1.0533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8836</th>\n",
       "      <td>252.9040</td>\n",
       "      <td>41.1097</td>\n",
       "      <td>1.1634</td>\n",
       "      <td>6.3707</td>\n",
       "      <td>1.1412</td>\n",
       "      <td>1.1021</td>\n",
       "      <td>1.0867</td>\n",
       "      <td>0.6259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10424</th>\n",
       "      <td>161.6176</td>\n",
       "      <td>74.3161</td>\n",
       "      <td>2.0614</td>\n",
       "      <td>11.7613</td>\n",
       "      <td>1.2930</td>\n",
       "      <td>1.5396</td>\n",
       "      <td>1.2642</td>\n",
       "      <td>1.2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3660</th>\n",
       "      <td>696.5560</td>\n",
       "      <td>48.2119</td>\n",
       "      <td>1.3284</td>\n",
       "      <td>7.6821</td>\n",
       "      <td>2.0032</td>\n",
       "      <td>1.2253</td>\n",
       "      <td>1.1575</td>\n",
       "      <td>0.7256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11964</th>\n",
       "      <td>419.1736</td>\n",
       "      <td>34.6981</td>\n",
       "      <td>0.8151</td>\n",
       "      <td>6.3539</td>\n",
       "      <td>1.5744</td>\n",
       "      <td>1.1573</td>\n",
       "      <td>1.1795</td>\n",
       "      <td>0.9840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5191</th>\n",
       "      <td>253.6428</td>\n",
       "      <td>62.2097</td>\n",
       "      <td>1.6736</td>\n",
       "      <td>10.1447</td>\n",
       "      <td>2.0540</td>\n",
       "      <td>1.1803</td>\n",
       "      <td>1.0450</td>\n",
       "      <td>0.6465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>286.8660</td>\n",
       "      <td>46.7980</td>\n",
       "      <td>1.4353</td>\n",
       "      <td>6.5992</td>\n",
       "      <td>1.2072</td>\n",
       "      <td>1.1399</td>\n",
       "      <td>0.7957</td>\n",
       "      <td>0.5089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>135.5716</td>\n",
       "      <td>46.5403</td>\n",
       "      <td>1.3413</td>\n",
       "      <td>7.0691</td>\n",
       "      <td>1.1543</td>\n",
       "      <td>1.2768</td>\n",
       "      <td>1.0558</td>\n",
       "      <td>0.6259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>3.1204</td>\n",
       "      <td>6.2592</td>\n",
       "      <td>0.1854</td>\n",
       "      <td>0.9210</td>\n",
       "      <td>0.5626</td>\n",
       "      <td>1950.0000</td>\n",
       "      <td>1765.0116</td>\n",
       "      <td>0.0915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8875 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       DEPTH_WMSF     HCGR     HFK     HTHO    HURA       IDPH       IMPH  \\\n",
       "7632      58.8988  11.0347  0.3173   1.6807  0.5729  1950.0000  1765.0116   \n",
       "11425    327.2764  50.1788  1.2883   8.5476  1.9532     1.2876     1.2937   \n",
       "8836     252.9040  41.1097  1.1634   6.3707  1.1412     1.1021     1.0867   \n",
       "10424    161.6176  74.3161  2.0614  11.7613  1.2930     1.5396     1.2642   \n",
       "3660     696.5560  48.2119  1.3284   7.6821  2.0032     1.2253     1.1575   \n",
       "...           ...      ...     ...      ...     ...        ...        ...   \n",
       "11964    419.1736  34.6981  0.8151   6.3539  1.5744     1.1573     1.1795   \n",
       "5191     253.6428  62.2097  1.6736  10.1447  2.0540     1.1803     1.0450   \n",
       "5390     286.8660  46.7980  1.4353   6.5992  1.2072     1.1399     0.7957   \n",
       "860      135.5716  46.5403  1.3413   7.0691  1.1543     1.2768     1.0558   \n",
       "7270       3.1204   6.2592  0.1854   0.9210  0.5626  1950.0000  1765.0116   \n",
       "\n",
       "         SFLU  \n",
       "7632   0.0915  \n",
       "11425  1.0533  \n",
       "8836   0.6259  \n",
       "10424  1.2019  \n",
       "3660   0.7256  \n",
       "...       ...  \n",
       "11964  0.9840  \n",
       "5191   0.6465  \n",
       "5390   0.5089  \n",
       "860    0.6259  \n",
       "7270   0.0915  \n",
       "\n",
       "[8875 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 2, 3, ..., 3, 3, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Separate the data into features (X) and the target variable (y)\n",
    "X = data.drop(columns = 'lithology')\n",
    "y = data['lithology']\n",
    "\n",
    "# Use as label encoder to convert the strings in the Lithology column to integers\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "# Remember - you will be marked based on the names chosen - please use the conventions of Python and ML\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, random_state = 42)\n",
    "\n",
    "display(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c667819",
   "metadata": {},
   "source": [
    "### Exercise 3.2 (20 minutes)\n",
    "\n",
    "Examine the training set. Are there any missing or unusual values in any of the columns? What are these values and in which columns can they be found? Use a Markdown cell to describe your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7323f4e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DEPTH_WMSF    0\n",
       "HCGR          6\n",
       "HFK           5\n",
       "HTHO          8\n",
       "HURA          2\n",
       "IDPH          1\n",
       "IMPH          8\n",
       "SFLU          3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67cd44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create barcharts to explore the data - there's clearly something unusual about IDPH, IMPH, and SFLU\n",
    "X_train.hist(figsize = (12,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc00b381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "X_train[['IDPH', 'IMPH', 'SFLU']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb55e7eb",
   "metadata": {},
   "source": [
    "We see that there are quite a few null values in the columns of the dataset. These will need to be removed prior to analysis. Since these are continuous variables, a good strategy would be to impute the mean value of each column in place of the null values.\n",
    "\n",
    "Looking at the bar charts of the columns in the data set, we see that IDPH, IMPH, and SFLU, have a very unusual distributions. This warrants further investigation using summary statistics. \n",
    "\n",
    "The summary statistics show that the maximum value of the IDPH and IMPH columns is 1950, while the maximum value of the SFLU column is 9700. However, you can see that the 75th percentile of those columns is only around 1–2. Consequently, values in these columns are likely to be referring to either missing data or invalid measurements. I would strongly consider discussing these values with the providers of this data in order to find out what exactly these values mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fcdd4d",
   "metadata": {},
   "source": [
    "## Exercise 3.3 (10 minutes)\n",
    "\n",
    "Replace any unusual values with `np.nan`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3a2299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the offending values using a lambda function - any other function that does the same thing will be \n",
    "# accepted as long as the procedure is explained in sufficient detail.\n",
    "X_train[['IDPH', 'IMPH']] = X_train[['IDPH', 'IMPH']].apply(lambda x: np.where(x == 1950, np.nan, x))\n",
    "X_train[['SFLU']] = X_train[['SFLU']].apply(lambda x: np.where(x == 9700, np.nan, x))\n",
    "\n",
    "# REMEMBER that you need to do this for the X_test dataset too!\n",
    "X_test[['IDPH', 'IMPH']] = X_test[['IDPH', 'IMPH']].apply(lambda x: np.where(x == 1950, np.nan, x))\n",
    "X_test[['SFLU']] = X_test[['SFLU']].apply(lambda x: np.where(x == 9700, np.nan, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b7f304",
   "metadata": {},
   "source": [
    "## Exercise 4 (30 mins)\n",
    "\n",
    "Create a pipeline with an `Imputer`, a `Scaler`, and a `DecisionTreeClassifier`. Set the `random_state` of the `DecisionTreeClassifier` to 42.\n",
    "\n",
    "Create and run a RandomizedSearchCV on three hyperparameters of your choice using `accuracy` as the metric of choice (use `n_iter = 20`). Explain what varying each of your selected hyperparameters will do to your model.\n",
    "\n",
    "Print out the accuracy and parameters of your best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74b9e98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END model__max_depth=8, model__max_features=2, model__min_samples_split=40; total time=   0.1s\n",
      "[CV 2/5] END model__max_depth=8, model__max_features=2, model__min_samples_split=40; total time=   0.1s\n",
      "[CV 3/5] END model__max_depth=8, model__max_features=2, model__min_samples_split=40; total time=   0.1s\n",
      "[CV 2/5] END model__max_depth=8, model__max_features=8, model__min_samples_split=60; total time=   0.1s\n",
      "[CV 5/5] END model__max_depth=8, model__max_features=8, model__min_samples_split=60; total time=   0.1s\n",
      "[CV 3/5] END model__max_depth=8, model__max_features=8, model__min_samples_split=60; total time=   0.1s\n",
      "[CV 1/5] END model__max_depth=8, model__max_features=8, model__min_samples_split=60; total time=   0.1s\n",
      "[CV 4/5] END model__max_depth=8, model__max_features=8, model__min_samples_split=60; total time=   0.1s\n",
      "[CV 4/5] END model__max_depth=8, model__max_features=2, model__min_samples_split=40; total time=   0.0s\n",
      "[CV 5/5] END model__max_depth=8, model__max_features=2, model__min_samples_split=40; total time=   0.0s\n",
      "[CV 1/5] END model__max_depth=4, model__max_features=6, model__min_samples_split=30; total time=   0.1s\n",
      "[CV 3/5] END model__max_depth=4, model__max_features=6, model__min_samples_split=30; total time=   0.0s\n",
      "[CV 2/5] END model__max_depth=4, model__max_features=6, model__min_samples_split=30; total time=   0.1s\n",
      "[CV 4/5] END model__max_depth=4, model__max_features=6, model__min_samples_split=30; total time=   0.0s\n",
      "[CV 5/5] END model__max_depth=4, model__max_features=6, model__min_samples_split=30; total time=   0.0s\n",
      "[CV 1/5] END model__max_depth=6, model__max_features=6, model__min_samples_split=60; total time=   0.1s\n",
      "[CV 3/5] END model__max_depth=6, model__max_features=6, model__min_samples_split=60; total time=   0.1s\n",
      "[CV 2/5] END model__max_depth=6, model__max_features=6, model__min_samples_split=60; total time=   0.1s\n",
      "[CV 4/5] END model__max_depth=6, model__max_features=6, model__min_samples_split=60; total time=   0.1s\n",
      "[CV 5/5] END model__max_depth=6, model__max_features=6, model__min_samples_split=60; total time=   0.1s\n",
      "[CV 1/5] END model__max_depth=9, model__max_features=5, model__min_samples_split=50; total time=   0.1s\n",
      "[CV 2/5] END model__max_depth=9, model__max_features=5, model__min_samples_split=50; total time=   0.1s\n",
      "[CV 3/5] END model__max_depth=9, model__max_features=5, model__min_samples_split=50; total time=   0.1s\n",
      "[CV 5/5] END model__max_depth=9, model__max_features=5, model__min_samples_split=50; total time=   0.1s\n",
      "[CV 4/5] END model__max_depth=9, model__max_features=5, model__min_samples_split=50; total time=   0.1s\n",
      "[CV 2/5] END model__max_depth=10, model__max_features=6, model__min_samples_split=40; total time=   0.1s\n",
      "[CV 1/5] END model__max_depth=10, model__max_features=6, model__min_samples_split=40; total time=   0.1s\n",
      "[CV 1/5] END model__max_depth=6, model__max_features=5, model__min_samples_split=40; total time=   0.1s\n",
      "[CV 2/5] END model__max_depth=6, model__max_features=5, model__min_samples_split=40; total time=   0.1s\n",
      "[CV 3/5] END model__max_depth=6, model__max_features=5, model__min_samples_split=40; total time=   0.1s\n",
      "[CV 3/5] END model__max_depth=10, model__max_features=6, model__min_samples_split=40; total time=   0.1s\n",
      "[CV 4/5] END model__max_depth=10, model__max_features=6, model__min_samples_split=40; total time=   0.1s\n",
      "[CV 5/5] END model__max_depth=6, model__max_features=5, model__min_samples_split=40; total time=   0.1s\n",
      "[CV 4/5] END model__max_depth=6, model__max_features=5, model__min_samples_split=40; total time=   0.1s\n",
      "[CV 5/5] END model__max_depth=10, model__max_features=6, model__min_samples_split=40; total time=   0.1s\n",
      "[CV 1/5] END model__max_depth=5, model__max_features=8, model__min_samples_split=60; total time=   0.1s\n",
      "[CV 1/5] END model__max_depth=4, model__max_features=6, model__min_samples_split=50; total time=   0.1s\n",
      "[CV 3/5] END model__max_depth=4, model__max_features=6, model__min_samples_split=50; total time=   0.1s\n",
      "[CV 3/5] END model__max_depth=5, model__max_features=8, model__min_samples_split=60; total time=   0.1s\n",
      "[CV 2/5] END model__max_depth=5, model__max_features=8, model__min_samples_split=60; total time=   0.1s\n",
      "[CV 2/5] END model__max_depth=4, model__max_features=6, model__min_samples_split=50; total time=   0.1s\n",
      "[CV 4/5] END model__max_depth=5, model__max_features=8, model__min_samples_split=60; total time=   0.1s\n",
      "[CV 5/5] END model__max_depth=5, model__max_features=8, model__min_samples_split=60; total time=   0.1s\n",
      "[CV 4/5] END model__max_depth=4, model__max_features=6, model__min_samples_split=50; total time=   0.2s\n",
      "[CV 1/5] END model__max_depth=10, model__max_features=2, model__min_samples_split=40; total time=   0.2s\n",
      "[CV 5/5] END model__max_depth=4, model__max_features=6, model__min_samples_split=50; total time=   0.2s\n",
      "[CV 2/5] END model__max_depth=10, model__max_features=2, model__min_samples_split=40; total time=   0.2s\n",
      "[CV 3/5] END model__max_depth=10, model__max_features=2, model__min_samples_split=40; total time=   0.2s\n",
      "[CV 4/5] END model__max_depth=10, model__max_features=2, model__min_samples_split=40; total time=   0.1s\n",
      "[CV 5/5] END model__max_depth=10, model__max_features=2, model__min_samples_split=40; total time=   0.2s\n",
      "[CV 1/5] END model__max_depth=4, model__max_features=5, model__min_samples_split=60; total time=   0.0s\n",
      "[CV 1/5] END model__max_depth=9, model__max_features=6, model__min_samples_split=30; total time=   0.1s\n",
      "[CV 2/5] END model__max_depth=4, model__max_features=5, model__min_samples_split=60; total time=   0.0s\n",
      "[CV 3/5] END model__max_depth=9, model__max_features=6, model__min_samples_split=30; total time=   0.1s\n",
      "[CV 2/5] END model__max_depth=9, model__max_features=6, model__min_samples_split=30; total time=   0.1s\n",
      "[CV 3/5] END model__max_depth=4, model__max_features=5, model__min_samples_split=60; total time=   0.0s\n",
      "[CV 4/5] END model__max_depth=9, model__max_features=6, model__min_samples_split=30; total time=   0.1s\n",
      "[CV 5/5] END model__max_depth=9, model__max_features=6, model__min_samples_split=30; total time=   0.1s\n",
      "[CV 5/5] END model__max_depth=4, model__max_features=5, model__min_samples_split=60; total time=   0.0s\n",
      "[CV 4/5] END model__max_depth=4, model__max_features=5, model__min_samples_split=60; total time=   0.0s\n",
      "[CV 1/5] END model__max_depth=6, model__max_features=3, model__min_samples_split=30; total time=   0.0s\n",
      "[CV 2/5] END model__max_depth=6, model__max_features=3, model__min_samples_split=30; total time=   0.0s\n",
      "[CV 3/5] END model__max_depth=6, model__max_features=3, model__min_samples_split=30; total time=   0.0s\n",
      "[CV 1/5] END model__max_depth=9, model__max_features=7, model__min_samples_split=40; total time=   0.1s\n",
      "[CV 4/5] END model__max_depth=9, model__max_features=7, model__min_samples_split=40; total time=   0.1s\n",
      "[CV 2/5] END model__max_depth=9, model__max_features=7, model__min_samples_split=40; total time=   0.1s\n",
      "[CV 3/5] END model__max_depth=9, model__max_features=7, model__min_samples_split=40; total time=   0.1s\n",
      "[CV 5/5] END model__max_depth=9, model__max_features=7, model__min_samples_split=40; total time=   0.1s\n",
      "[CV 4/5] END model__max_depth=6, model__max_features=3, model__min_samples_split=30; total time=   0.1s\n",
      "[CV 5/5] END model__max_depth=6, model__max_features=3, model__min_samples_split=30; total time=   0.0s\n",
      "[CV 1/5] END model__max_depth=8, model__max_features=2, model__min_samples_split=60; total time=   0.0s\n",
      "[CV 2/5] END model__max_depth=8, model__max_features=2, model__min_samples_split=60; total time=   0.0s\n",
      "[CV 3/5] END model__max_depth=8, model__max_features=2, model__min_samples_split=60; total time=   0.0s\n",
      "[CV 4/5] END model__max_depth=8, model__max_features=2, model__min_samples_split=60; total time=   0.0s\n",
      "[CV 5/5] END model__max_depth=8, model__max_features=2, model__min_samples_split=60; total time=   0.0s\n",
      "[CV 1/5] END model__max_depth=8, model__max_features=2, model__min_samples_split=50; total time=   0.0s\n",
      "[CV 2/5] END model__max_depth=8, model__max_features=2, model__min_samples_split=50; total time=   0.0s\n",
      "[CV 3/5] END model__max_depth=8, model__max_features=2, model__min_samples_split=50; total time=   0.0s\n",
      "[CV 4/5] END model__max_depth=8, model__max_features=2, model__min_samples_split=50; total time=   0.0s\n",
      "[CV 5/5] END model__max_depth=8, model__max_features=2, model__min_samples_split=50; total time=   0.0s\n",
      "[CV 1/5] END model__max_depth=9, model__max_features=3, model__min_samples_split=40; total time=   0.1s\n",
      "[CV 3/5] END model__max_depth=9, model__max_features=3, model__min_samples_split=40; total time=   0.0s\n",
      "[CV 2/5] END model__max_depth=4, model__max_features=4, model__min_samples_split=40; total time=   0.0s\n",
      "[CV 5/5] END model__max_depth=9, model__max_features=3, model__min_samples_split=40; total time=   0.1s\n",
      "[CV 4/5] END model__max_depth=4, model__max_features=4, model__min_samples_split=40; total time=   0.0s\n",
      "[CV 1/5] END model__max_depth=6, model__max_features=6, model__min_samples_split=40; total time=   0.1s\n",
      "[CV 3/5] END model__max_depth=4, model__max_features=4, model__min_samples_split=40; total time=   0.1s\n",
      "[CV 2/5] END model__max_depth=9, model__max_features=3, model__min_samples_split=40; total time=   0.1s\n",
      "[CV 4/5] END model__max_depth=9, model__max_features=3, model__min_samples_split=40; total time=   0.1s\n",
      "[CV 1/5] END model__max_depth=4, model__max_features=4, model__min_samples_split=40; total time=   0.1s\n",
      "[CV 5/5] END model__max_depth=4, model__max_features=4, model__min_samples_split=40; total time=   0.1s\n",
      "[CV 3/5] END model__max_depth=6, model__max_features=6, model__min_samples_split=40; total time=   0.1s\n",
      "[CV 5/5] END model__max_depth=6, model__max_features=6, model__min_samples_split=40; total time=   0.1s\n",
      "[CV 2/5] END model__max_depth=6, model__max_features=6, model__min_samples_split=40; total time=   0.1s\n",
      "[CV 4/5] END model__max_depth=6, model__max_features=6, model__min_samples_split=40; total time=   0.1s\n",
      "[CV 2/5] END model__max_depth=10, model__max_features=6, model__min_samples_split=60; total time=   0.1s\n",
      "[CV 3/5] END model__max_depth=10, model__max_features=6, model__min_samples_split=60; total time=   0.1s\n",
      "[CV 1/5] END model__max_depth=10, model__max_features=6, model__min_samples_split=60; total time=   0.1s\n",
      "[CV 5/5] END model__max_depth=10, model__max_features=6, model__min_samples_split=60; total time=   0.1s\n",
      "[CV 4/5] END model__max_depth=10, model__max_features=6, model__min_samples_split=60; total time=   0.1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=Pipeline(steps=[('imputer', SimpleImputer()),\n",
       "                                             ('scaler', StandardScaler()),\n",
       "                                             ('model',\n",
       "                                              DecisionTreeClassifier(random_state=42))]),\n",
       "                   n_iter=20, n_jobs=-1,\n",
       "                   param_distributions={'model__max_depth': [4, 5, 6, 7, 8, 9,\n",
       "                                                             10],\n",
       "                                        'model__max_features': [2, 3, 4, 5, 6,\n",
       "                                                                7, 8],\n",
       "                                        'model__min_samples_split': [30, 40, 50,\n",
       "                                                                     60]},\n",
       "                   random_state=42, scoring='accuracy', verbose=5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Create decision tree pipeline. Note that we set random_state = 42\n",
    "dt_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', DecisionTreeClassifier(random_state = 42))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'model__max_features': [i for i in range(2, 9)],\n",
    "    'model__max_depth': [i for i in range(4, 11)],\n",
    "    'model__min_samples_split': [i*10 for i in range(3, 7)]\n",
    "}\n",
    "\n",
    "dt_search = RandomizedSearchCV(\n",
    "    dt_pipe, \n",
    "    param_distributions=param_grid,\n",
    "    n_iter=20,\n",
    "    n_jobs=-1,\n",
    "    verbose=5,\n",
    "    random_state=42,\n",
    "    scoring = 'accuracy'\n",
    ")\n",
    "\n",
    "dt_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "754940fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('imputer', SimpleImputer()), ('scaler', StandardScaler()),\n",
      "                ('model',\n",
      "                 DecisionTreeClassifier(max_depth=9, max_features=6,\n",
      "                                        min_samples_split=30,\n",
      "                                        random_state=42))])\n",
      "0.8812394366197183\n"
     ]
    }
   ],
   "source": [
    "print(dt_search.best_estimator_)\n",
    "print(dt_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803c55f2",
   "metadata": {},
   "source": [
    "## Exercise 5 (10 mins)\n",
    "\n",
    "Explain why accuracy may not be the best metric for assessing the performance of a classifier model.\n",
    "\n",
    "Describe three other classification metrics and the scenarios in which they would be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d635b6",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "**Note**: All of the answers here would be significantly improved by the presentation of an appropriate confusion matrix. You can see examples of such matrices in the lecture notes in Lecture 5, Notebook 2. This answer would also be improved by giving examples of scenarios where each metric would be useful.\n",
    "\n",
    "Accuracy may not be an ideal measure when the dataset is imbalanced, as it tends to significantly overestimate the performance of a model. \n",
    "\n",
    "For example, let's say that you have a binary target variable that is either \"Yes\" or \"No\". Let's say that your dataset had 99 \"Yes\" values and 1 \"No\" value. If you created a \"model\" that simply predicted \"Yes\" for each sample, regardless of the values in the features, your \"model\" would have an accuracy of 99% - but this would be a terrible model.\n",
    "\n",
    "Alternative classification metrics are as follows:\n",
    "\n",
    "#### 1. Recall\n",
    "\n",
    "Recall is calculated using the following formula:\n",
    "   \n",
    "$$recall = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "This metric measures your model's ability to detect occurrences of the positive class. In other words, it's very useful in scenarios where you want to ensure that you identify as many \"Yes\" values as possible.\n",
    "\n",
    "#### 2. Precision\n",
    "\n",
    "Precision is calculated using the following formula:\n",
    "\n",
    "$$precision = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "This metric measures your model's ability to correctly identify True Positives. In other words, if your model says that a sample is \"True\", this metric tells you how confident you should be that the model has really detected a True Positive rather than a False Positive. \n",
    "\n",
    "This is useful in scenarios where you want to be sure that a positive result really is positive. For example, if you are hunting for gold, and your model tells you that a lump of rock contains a lot of gold in it, then you want to be very certain that it contains gold before spending all your money processing the rock. \n",
    "\n",
    "#### 3. F1-Score\n",
    "\n",
    "The F1-score combines precision and recall into a generic score. \n",
    "\n",
    "$$F_1=2x\\frac{precision \\times recall}{precision + recall}$$\n",
    "\n",
    "This metric is good if you want a generically good model. The primary downside is that this value is difficult to explain in layman's terms. Other metrics may also be more suited for specific use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459cc8de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
